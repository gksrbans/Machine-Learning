# Linear Regression
- 학습 데이터를 가지고 근사값(?)을 찾는 모델을 만드는 것.
- (Linear) Hypothesis : 알맞은 선을 찾는 과정
- Which hypothesis is better? : H(x)  = Wx + b  
ㄴ Cost(Loss) function : 우리가 세운 가설과 실제 데이터와의 gap 차이.
- 차이가 양수 일수도, 음수 일수도 있기 때문에 제곱으로 표현함. $$ (H(x) - y)^{2}$$

### Goal
- Cost의 최솟값을 구하는 것이 학습의 목표가 됌.
- minimize cost(W, b)
- 이렇게 최소 비용을 구하는 많은 알고리즘이 개발이 되어있음.

```sh
#import tensorflow as tf
import tensorflow.compat.v1 as tf
tf.disable_v2_behavior()

# tf.__version__

x_train = [1,2,3]
y_train = [1,2,3]

W = tf.Variable(tf.random_normal([1]), name='weight')
b = tf.Variable(tf.random_normal([1]), name='bias')

# 우리의 가설
hypothesis = x_train * W + b
cost = tf.reduce_mean(tf.square(hypothesis - y_train))
# reduce_mean = 평균내줌

#GradientDesceent
optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)
train = optimizer.minimize(cost)

sess = tf.Session()
sess.run(tf.global_variables_initializer())

for step in range(2001):
  sess.run(train)
  if step % 20 == 0:
    print(step, sess.run(cost), sess.run(W), sess.run(b))
```
 출력 결과
 >0 14.53403 [-0.27668086] [-1.1137108]
20 0.15937708 [1.052986] [-0.50284153]
40 0.02651281 [1.1713927] [-0.42602703]
60 0.023007331 [1.1748495] [-0.40094122]
80 0.020885902 [1.1677284] [-0.3816162]
100 0.018968813 [1.1599501] [-0.36363575]
120 0.0172278 [1.152443] [-0.34654188]
140 0.015646556 [1.1452796] [-0.3302553]
160 0.014210458 [1.1384522] [-0.3147344]
180 0.012906154 [1.1319455] [-0.2999431]
200 0.011721589 [1.1257446] [-0.28584692]
220 0.010645739 [1.119835] [-0.27241313]
240 0.009668623 [1.1142032] [-0.2596107]
260 0.008781205 [1.1088362] [-0.24741003]
280 0.00797523 [1.1037211] [-0.23578271]
300 0.0072432286 [1.0988466] [-0.22470166]
320 0.006578417 [1.0942012] [-0.21414149]
340 0.005974621 [1.0897741] [-0.20407766]
360 0.0054262467 [1.0855551] [-0.19448677]
380 0.0049282145 [1.0815345] [-0.18534663]
400 0.0044758786 [1.0777025] [-0.17663608]
420 0.004065062 [1.0740508] [-0.16833489]
440 0.0036919585 [1.0705707] [-0.16042376]
460 0.003353101 [1.0672542] [-0.15288444]
480 0.0030453342 [1.0640935] [-0.14569949]
500 0.0027658253 [1.061081] [-0.13885218]
520 0.002511961 [1.0582106] [-0.13232648]
540 0.0022814034 [1.055475] [-0.1261076]
560 0.0020720076 [1.0528679] [-0.12018103]
580 0.0018818328 [1.0503833] [-0.11453303]
600 0.0017091087 [1.0480155] [-0.10915044]
620 0.0015522431 [1.045759] [-0.10402077]
640 0.0014097717 [1.0436084] [-0.09913218]
660 0.001280381 [1.0415592] [-0.0944734]
680 0.0011628631 [1.039606] [-0.09003368]
700 0.0010561311 [1.0377445] [-0.08580239]
720 0.00095919846 [1.0359708] [-0.08177]
740 0.000871158 [1.0342803] [-0.07792713]
760 0.0007912006 [1.0326693] [-0.07426488]
780 0.00071858027 [1.031134] [-0.07077474]
800 0.0006526277 [1.0296707] [-0.06744864]
820 0.0005927254 [1.0282763] [-0.0642788]
840 0.00053832476 [1.0269475] [-0.06125795]
860 0.0004889176 [1.0256811] [-0.05837908]
880 0.00044404148 [1.0244741] [-0.05563552]
900 0.000403286 [1.023324] [-0.05302085]
920 0.00036627243 [1.022228] [-0.05052914]
940 0.00033265608 [1.0211835] [-0.04815454]
960 0.00030212657 [1.0201881] [-0.04589166]
980 0.00027439542 [1.019239] [-0.04373502]
1000 0.00024920967 [1.0183349] [-0.04167954]
1020 0.00022633596 [1.0174732] [-0.03972071]
1040 0.0002055606 [1.016652] [-0.03785397]
1060 0.00018669397 [1.0158694] [-0.03607494]
1080 0.00016955832 [1.0151237] [-0.03437958]
1100 0.00015399764 [1.014413] [-0.03276391]
1120 0.00013986207 [1.0137357] [-0.03122418]
1140 0.00012702493 [1.01309] [-0.02975681]
1160 0.00011536616 [1.0124749] [-0.02835834]
1180 0.00010477853 [1.0118886] [-0.02702557]
1200 9.5161e-05 [1.0113299] [-0.02575549]
1220 8.6427135e-05 [1.0107975] [-0.02454511]
1240 7.8493795e-05 [1.01029] [-0.02339157]
1260 7.128988e-05 [1.0098063] [-0.02229222]
1280 6.474625e-05 [1.0093455] [-0.02124455]
1300 5.880371e-05 [1.0089064] [-0.02024614]
1320 5.3406326e-05 [1.0084877] [-0.01929466]
1340 4.8504506e-05 [1.0080888] [-0.01838788]
1360 4.40529e-05 [1.0077087] [-0.01752369]
1380 4.0009036e-05 [1.0073465] [-0.01670017]
1400 3.6337336e-05 [1.0070012] [-0.01591539]
1420 3.3002154e-05 [1.0066721] [-0.01516742]
1440 2.99724e-05 [1.0063585] [-0.01445455]
1460 2.7222113e-05 [1.0060598] [-0.01377521]
1480 2.4723331e-05 [1.005775] [-0.01312785]
1500 2.2453809e-05 [1.0055035] [-0.01251091]
1520 2.0393272e-05 [1.005245] [-0.01192293]
1540 1.852148e-05 [1.0049984] [-0.0113626]
1560 1.6821112e-05 [1.0047635] [-0.01082858]
1580 1.5277648e-05 [1.0045396] [-0.01031966]
1600 1.38752175e-05 [1.0043263] [-0.00983466]
1620 1.2601736e-05 [1.004123] [-0.0093725]
1640 1.1444935e-05 [1.0039291] [-0.00893199]
1660 1.039421e-05 [1.0037445] [-0.00851219]
1680 9.440561e-06 [1.0035686] [-0.00811218]
1700 8.574358e-06 [1.0034009] [-0.00773099]
1720 7.787256e-06 [1.0032411] [-0.00736771]
1740 7.072457e-06 [1.0030888] [-0.00702149]
1760 6.4236774e-06 [1.0029436] [-0.00669154]
1780 5.8342325e-06 [1.0028054] [-0.00637713]
1800 5.298761e-06 [1.0026736] [-0.00607749]
1820 4.8125303e-06 [1.0025479] [-0.00579189]
1840 4.370874e-06 [1.0024282] [-0.00551971]
1860 3.969643e-06 [1.0023141] [-0.00526035]
1880 3.6051606e-06 [1.0022053] [-0.00501316]
1900 3.274475e-06 [1.0021017] [-0.00477762]
1920 2.9739977e-06 [1.002003] [-0.00455313]
1940 2.701069e-06 [1.0019088] [-0.0043392]
1960 2.4534118e-06 [1.0018193] [-0.00413535]
1980 2.2281774e-06 [1.0017338] [-0.00394104]
2000 2.023673e-06 [1.0016522] [-0.00375586]

- 처음 실행 때는 Cost값이 크나, 2000번의 학습이 진행될 수록 w값이 1에 수렴하며, b는 0에 가까워지는 결과가 도출....

#### placeholder를 사용하면 리스트화 해서 feed_dict로 값을 입력받아 사용 가능함.
```sh
#import tensorflow as tf
import tensorflow.compat.v1 as tf
tf.disable_v2_behavior()

# tf.__version__

# x_train = [1,2,3]
# y_train = [1,2,3]

x = tf.placeholder(tf.float32)
y = tf.placeholder(tf.float32)

W = tf.Variable(tf.random_normal([1]), name='weight')
b = tf.Variable(tf.random_normal([1]), name='bias')

# 우리의 가설
hypothesis = x_train * W + b
cost = tf.reduce_mean(tf.square(hypothesis - y_train))
# reduce_mean = 평균내줌

#GradientDesceent
optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)
train = optimizer.minimize(cost)

sess = tf.Session()
sess.run(tf.global_variables_initializer())

# for step in range(2001):
#   sess.run(train)
#   if step % 20 == 0:
#     print(step, sess.run(cost), sess.run(W), sess.run(b))


for step in range(2001):
  cost_val, W_val, b_val, _ = \
    sess.run([cost, W, b, train], 
             feed_dict={x: [1, 2, 3], y: [1, 2, 3]})
  if step % 20 == 0: print(step, cost_val, W_val, b_val)
```
